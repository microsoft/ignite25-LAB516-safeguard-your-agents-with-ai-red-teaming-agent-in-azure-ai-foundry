# Quick Demo

This guide explains how to set up and run the AI Red Teaming Agent script with your existing Azure Developer CLI (azd) deployment from the `.infra-setup` directory.

## Overview

The AI Red Teaming Agent is a powerful tool designed to help organizations proactively find security and safety risks associated with generative AI systems during design and development. It performs automated security and safety scans on your deployed AI agent to assess your risk posture before deploying to production.

## Prerequisites

1. **Completed azd deployment**: You must have successfully deployed the AI agent solution using `azd up` from the `.infra-setup` directory
2. **Python 3.9+**: Ensure Python is installed and available in your environment
3. **Azure credentials**: Proper Azure authentication set up (handled by azd)

## Required Dependencies

Before running the red teaming script, install the required packages:

```bash
# Navigate to the .infra-setup directory
cd .infra-setup

# Install base requirements
python -m pip install -r src/requirements.txt

# Install additional red teaming dependencies
python -m pip install azure-ai-evaluation[redteam]
```

## Environment Variables

The red teaming script requires several environment variables that are automatically generated when you run `azd up`. These are stored in the `.infra-setup/src/.env` file after deployment.

### Required Environment Variables

| Variable | Description | Source |
|----------|-------------|---------|
| `AZURE_EXISTING_AIPROJECT_ENDPOINT` | AI Project endpoint URL | Generated by azd deployment |
| `AZURE_EXISTING_AGENT_ID` | The specific agent ID to test | Generated remotely in Container App |
| `AZURE_AI_AGENT_NAME` | Agent name (fallback if Agent ID not set) | Generated by azd deployment |
| `AZURE_AI_AGENT_DEPLOYMENT_NAME` | Model deployment name used by evaluators | Generated by azd deployment |

### Finding Your Agent ID

The `AZURE_EXISTING_AGENT_ID` is the one variable that may need manual retrieval:

1. Go to [Azure AI Foundry Portal](https://ai.azure.com/) and sign in
2. Click on your project from the homepage
3. In the left-hand menu, select **Agents**
4. Choose the agent you want to test
5. The Agent ID will be shown in the agent's detail panel

Alternatively, you can rely on the `AZURE_AI_AGENT_NAME` variable, and the script will automatically look up the agent ID.

### Setting Environment Variables

After `azd up` completes, the environment variables are automatically written to `.infra-setup/src/.env`. If you need to manually set any variables:

```bash
# From the .infra-setup directory
azd env set AZURE_EXISTING_AGENT_ID "your-agent-id-here"

# Or export directly for the current session
export AZURE_EXISTING_AGENT_ID="your-agent-id-here"
```

## Running the Red Teaming Script

1. **Navigate to the infra-setup directory**:
   ```bash
   cd .infra-setup
   ```

2. **Verify environment variables** (optional):
   ```bash
   # Check if the .env file exists and contains the required variables
   cat src/.env
   ```

3. **Run the red teaming script**:
   ```bash
   python airedteaming/ai_redteaming.py
   ```

## Script Configuration

The red teaming script is currently configured with:

- **Risk Categories**: Violence (can be customized)
- **Attack Strategies**: Flip (can be customized)  
- **Number of Objectives**: 1 (can be customized)
- **Output Directory**: `redteam_outputs/`

### Customizing the Scan

You can modify the `airedteaming/ai_redteaming.py` script to adjust:

- **Risk Categories**: Choose from available options like `RiskCategory.Hate`, `RiskCategory.SelfHarm`, etc.
- **Attack Strategies**: Select different strategies like `AttackStrategy.DirectRequest`, `AttackStrategy.Jailbreak`, etc.
- **Number of Objectives**: Increase for more comprehensive testing
- **Output Directory**: Change where results are saved

Example customization:
```python
red_team = RedTeam(
    azure_ai_project=project_endpoint,
    credential=credential,
    risk_categories=[RiskCategory.Violence, RiskCategory.Hate, RiskCategory.SelfHarm],
    num_objectives=5,  # Increased for more thorough testing
    output_dir="custom_redteam_outputs/"
)
```

## Expected Output

The script will:

1. Load environment variables from the `.env` file
2. Connect to your Azure AI project using managed identity
3. Locate your deployed agent (by ID or name)
4. Display agent details for verification
5. Run the red teaming scan with generated attack prompts
6. Save results to the output directory

## Results Analysis

After the scan completes, results will be saved in the `redteam_outputs/` directory (or your custom output directory). The results include:

- Attack success rates by risk category
- Detailed logs of attack attempts
- Safety evaluation scores
- Recommendations for improving agent security

## Troubleshooting

### Common Issues

1. **Missing Agent ID**: 
   - Ensure `AZURE_EXISTING_AGENT_ID` is set, or verify `AZURE_AI_AGENT_NAME` matches your deployed agent

2. **Authentication Errors**:
   - Make sure you're authenticated with Azure: `az login`
   - Verify your account has access to the AI project

3. **Environment Variables Not Found**:
   - Ensure you've run `azd up` successfully
   - Check that `src/.env` file exists and contains the required variables

4. **Module Import Errors**:
   - Verify all dependencies are installed: `pip install azure-ai-evaluation[redteam]`

### Getting Help

For more detailed information about:
- **Attack Techniques and Risk Categories**: [Azure AI Red Teaming Documentation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/run-scans-ai-red-teaming-agent)
- **Azure AI Foundry**: [AI Foundry Documentation](https://learn.microsoft.com/azure/ai-foundry/)
- **General Troubleshooting**: See the troubleshooting guide in `.infra-setup/docs/troubleshooting.md`

## Security Considerations

- The red teaming scan generates adversarial prompts to test your agent's safety
- Results may contain sensitive information - handle output files appropriately
- Use this tool during development/testing phases before production deployment
- Review and address any identified vulnerabilities before deploying to production

## Cost Considerations

The red teaming scan incurs costs based on:
- Azure AI Risk and Safety Evaluations consumption
- Model usage during the scanning process
- See the [Azure AI Foundry pricing page](https://azure.microsoft.com/pricing/details/ai-foundry/) for current rates

## Next Steps

After running the red teaming scan:

1. Review the generated reports in the output directory
2. Address any identified security or safety issues
3. Re-run scans after making improvements
4. Consider integrating red teaming into your CI/CD pipeline for continuous security validation